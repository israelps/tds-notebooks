{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enclosed-bundle",
   "metadata": {},
   "source": [
    "#### 1. Create a Local Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "general-external",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b5f6f4db32f4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kmeans-seed-dataset</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f751ce0fc10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"kmeans-seed-dataset\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-threshold",
   "metadata": {},
   "source": [
    "#### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "frequent-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for ML pipeline and training\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-pastor",
   "metadata": {},
   "source": [
    "#### 3. Import Seeds dataset from a .csv file\n",
    "This dataset is from UCI Machine Learning Repository called: Seeds Dataset: Measurements of geometrical properties of kernels belonging to three different varieties of wheat. A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.\n",
    "link: https://archive.ics.uci.edu/ml/datasets/seeds\n",
    "\n",
    "The real data waas a .txt file with no columns names, I transform to a .csv data with columns names to import on Spark DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wound-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset from csv file\n",
    "dataset = spark.read.csv(\"seeds_dataset.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "transparent-arrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(A=15.26, P=14.84, C=0.871, LK=5.763, WK=3.312, A_Coef=2.221, LKG=5.22, target=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-burke",
   "metadata": {},
   "source": [
    "#### 4. Create a Data pre-processing Pipeline \n",
    "#####   4.1. Create a vector from all features columns\n",
    "#####   4.2. Standardize data\n",
    "#####   4.3. Pass this data to a KMeans object and create the Pipeline object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fitted-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols = dataset.columns[:-1], outputCol='features')\n",
    "\n",
    "scaler = StandardScaler(inputCol=vec_assembler.getOutputCol(), outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "kmeans = KMeans(featuresCol=scaler.getOutputCol()).setK(2).setSeed(1)\n",
    "\n",
    "pipeline = Pipeline(stages=[vec_assembler,scaler,kmeans])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-plaza",
   "metadata": {},
   "source": [
    "#### 5. Fit and Evaluate the model\n",
    "\n",
    "##### The ClusteringEvaluator use the Silhoute metric do evaluate the model\n",
    "  \n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster. ref: https://en.wikipedia.org/wiki/Silhouette_(clustering)\n",
    "\n",
    "Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis\". Computational and Applied Mathematics. 20: 53â€“65. doi:10.1016/0377-0427(87)90125-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defensive-california",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.7098457795960431\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(dataset)\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-feature",
   "metadata": {},
   "source": [
    "##### 6. Get the cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designed-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 6.2407035  12.29350122 37.40324608 13.82968554  9.69123508  2.31478858\n",
      " 12.15051313]\n",
      "[ 4.44396468 10.48536862 36.54671035 12.05177027  8.0111241   2.5455929\n",
      " 10.33965102]\n"
     ]
    }
   ],
   "source": [
    "centers = model.stages[2].clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-decline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
